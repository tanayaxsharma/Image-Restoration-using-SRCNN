{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b189974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "from noggin import create_plot\n",
    "import os\n",
    "import skimage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4226767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving jw-image-1.jpg\n",
      "Saving jw-image-2.jpg\n",
      "Saving jw-image-3.jpg\n",
      "Saving jw-image-4.jpg\n"
     ]
    }
   ],
   "source": [
    "def interpolation(path, factor):\n",
    "    for file in os.listdir(path):\n",
    "        img = cv2.imread(path + '/' + file)\n",
    "        h, w, c = img.shape\n",
    "        new_height = int(h / factor)\n",
    "        new_width = int(w / factor)\n",
    "        \n",
    "        # resize the image - down\n",
    "        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR) #interploation are methods for resizing images;how do you go from image with 100px to 1000px \n",
    "        #bilinear interpolation\n",
    "        \n",
    "        # resize the image - up\n",
    "        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        print('Saving {}'.format(file))\n",
    "        cv2.imwrite('bad/{}'.format(file), img)\n",
    "\n",
    "interpolation(\"imgs/\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86df889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(target,ref):\n",
    "    # the MSE between the two images is the sum of the squared difference between the two images\n",
    "    err=np.sum((target.astype('float')-ref.astype('float'))**2)\n",
    "    err=err/float(target.shape[0]*target.shape[1])#divided by total number of pixels\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a261bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jw-image-1.jpg\n",
      "MSE: 377.30517054056463\n",
      "\n",
      "jw-image-2.jpg\n",
      "MSE: 125.78888088273251\n",
      "\n",
      "jw-image-3.jpg\n",
      "MSE: 220.87953881527062\n",
      "\n",
      "jw-image-4.jpg\n",
      "MSE: 153.04048387504105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('bad/'):\n",
    "    \n",
    "    #open target and reference images\n",
    "    target=cv2.imread('bad/{}'.format(file))\n",
    "    ref = cv2.imread('imgs/{}'.format(file))\n",
    "    \n",
    "    #calculate scores\n",
    "    score = mse(target, ref)\n",
    "    \n",
    "    print('{}\\nMSE: {}\\n'.format(file, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7738d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_res():\n",
    "    SRCNN = Sequential()\n",
    "        \n",
    "    SRCNN.add(Conv2D(filters=128, kernel_size = (9, 9), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='valid', use_bias=True, input_shape=(None, None, 1)))#only if in keras.json image_data_format is channels_last; else if channels_first then 1,None,None\n",
    "        \n",
    "    SRCNN.add(Conv2D(filters=64, kernel_size = (3, 3), kernel_initializer='glorot_uniform',\n",
    "                         activation='relu', padding='same', use_bias=True))\n",
    "        \n",
    "    SRCNN.add(Conv2D(filters=1, kernel_size = (5, 5), kernel_initializer='glorot_uniform',\n",
    "                         activation='linear', padding='valid', use_bias=True))\n",
    "        \n",
    "    SRCNN.compile(optimizer=Adam(lr=0.0003), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f48e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modcrop(img,scale):\n",
    "    #temp size\n",
    "    tmpsz=img.shape\n",
    "    sz=tmpsz[0:2]\n",
    "    \n",
    "    #ensures that dimension of our image are divisible by scale(doesn't leaves hanging remainders) by cropping the images size\n",
    "    #np.mod returns the remainder bewtween our sz and scale\n",
    "    sz=sz-np.mod(sz,scale)\n",
    "    \n",
    "    img=img[0:sz[0],1:sz[1]]\n",
    "    return img\n",
    "\n",
    "#crop offs the bordersize from all sides of the image\n",
    "def shave(image,border):\n",
    "    img=image[border: -border,border:-border]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8334572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main prediction function\n",
    "\n",
    "def predict(image_path):\n",
    "    \n",
    "    #load the srcnn model with weights cuz deep learning neural n/w take lot of time to train(have to feed in large amount of input data)\n",
    "    srcnn=super_res()\n",
    "    srcnn.load_weights('3051crop_weight_200.h5')\n",
    "     \n",
    "    #load the degraded and reference images\n",
    "    #in opencv, images are loaded as BGR channels\n",
    "    path,file=os.path.split(image_path)\n",
    "    degraded=cv2.imread(image_path)\n",
    "    ref=cv2.imread('source/{}'.format(file))\n",
    "    \n",
    "    #preprocess the image with modcrop\n",
    "#     ref=modcrop(ref,3)#when calculating our image quality metrics later we have the same size image to what we produce in SRCNN network\n",
    "#     degraded=modcrop(degraded,3)\n",
    "    \n",
    "    #convert the image to YCrCb(3 channel image) - (srcnn trained on Y channel)\n",
    "    temp=cv2.cvtColor(degraded,cv2.COLOR_BGR2YCrCb)\n",
    "    #opencv does a very good job in converting from rgb to YCrCb and back\n",
    "    \n",
    "    #create image slice and normalize cuz SRCNN works on one dimensional input(or 3D inputs of depth 1 ,ie, inputs with one channel)\n",
    "    Y=numpy.zeros((1,temp.shape[0],temp.shape[1],1),dtype=float)\n",
    "    #create a numpy array the we fill with data,temp.shape[0]=width,[1]=height and last one means one channel(essentially like batch=1 cuz that's what going to get passed to the n/w ')\n",
    "    #fill in the data; all values are normalized to between 0 and 1 as that's how srcnn was trained\n",
    "    Y[0,:,:,0]=temp[:,:,0].astype(float)/255\n",
    "    #first 0 means 0th index(we are saying that batch size is 1); :,: means every point in these channels; last 0 means first channel,ie, all the pixels in first luminescence channel\n",
    "    #so we have our image slice, we have the Y channel, which is the first channel(index 0) out of the image that we converted to YCrCb color space\n",
    "    \n",
    "    #perform super-resolution with srcnn\n",
    "    pre=srcnn.predict(Y,batch_size=1)#that's why we had index 0  above cuz we are saying that batch size is 1\n",
    "    \n",
    "    #post-process output cuz pre is still normalized\n",
    "    pre*=255#multiplying every pixel by 255\n",
    "    pre[pre[:]>255]=255#any pixels >255 set it =255 to prevent any rounding errors due to multiplication\n",
    "    pre[pre[:]<0] =0# same reason as above\n",
    "    pre=pre.astype(np.uint8)#convert float back to int values\n",
    "    \n",
    "    #cuz this is only the luminescence channel in the pre ,SO\n",
    "    #copy Y channel back to image and convert to BGR\n",
    "    temp=shave(temp,6)#accd.to tutor it loses 3 pixels on each side so if we shave this with a border 6,we can crop it appropriately there, so it is the same size as our output\n",
    "    #if not agree with tutor, use print statements to see the specific dimensions\n",
    "    \n",
    "    # for the first channel(Y channel), copy in the output of our network\n",
    "    temp[:,:,0]=pre[0,:,:,0]\n",
    "    #So we are keeping the red difference and blue difference, channels 1 and 2, in this temp image which is in the YCrCb color space\n",
    "    #and in the first one we are copying in our ouput,our luminiscence channel\n",
    "    \n",
    "    #convert back to bgr\n",
    "    output=cv2.cvtColor(temp,cv2.COLOR_YCrCb2BGR)\n",
    "    \n",
    "    #emove borderfrom reference and degraded image, so that all our images(ref,degraded(low res.), and ouput(high res.)) are of the same size\n",
    "#     ref = shave(ref.astype(np.uint8), 6)\n",
    "#     degraded = shave(degraded.astype(np.uint8), 6)\n",
    "    \n",
    "    # image quality calculations\n",
    "    scores = []\n",
    "    scores.append(mse(degraded, ref))#degraded wrt ref\n",
    "    scores.append(mse(output, ref))#high res. output wrt ref\n",
    "    \n",
    "    # return images and scores\n",
    "    return output, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "668281b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\bld\\libopencv_1657598065368\\work\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output,scores\u001b[38;5;241m=\u001b[39m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbad/jw-image-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print all scores for all images\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDegraded Image: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m     ref\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#preprocess the image with modcrop\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     ref=modcrop(ref,3)#when calculating our image quality metrics later we have the same size image to what we produce in SRCNN network\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     degraded=modcrop(degraded,3)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#convert the image to YCrCb(3 channel image) - (srcnn trained on Y channel)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     temp\u001b[38;5;241m=\u001b[39m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdegraded\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2YCrCb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#opencv does a very good job in converting from rgb to YCrCb and back\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#create image slice and normalize cuz SRCNN works on one dimensional input(or 3D inputs of depth 1 ,ie, inputs with one channel)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     Y\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,temp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],temp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m),dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\bld\\libopencv_1657598065368\\work\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "output,scores=predict('bad/jw-image-1')\n",
    "# print all scores for all images\n",
    "print('Degraded Image: \\nMSE: {}\\n'.format(scores[0][0]))\n",
    "print('Reconstructed Image: \\nMSE: {}\\n'.format(scores[1][0]))\n",
    "\n",
    "\n",
    "# display images as subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 8))#1 row,3 columns\n",
    "axs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))#first subplot\n",
    "#imshow assumes RGB images but cv2 loads images as BGR else channel mixing will take place and we will get weird images\n",
    "axs[0].set_title('Original')\n",
    "axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))#2nd subplot\n",
    "axs[1].set_title('Degraded')\n",
    "axs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "axs[2].set_title('SRCNN')\n",
    "\n",
    "# remove the x and y ticks\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])#leave them blank to remove ticks\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce8b86ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbad/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# perform super-resolution\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     ref, degraded, output \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbad/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# display images as subplots\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m ref\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#preprocess the image with modcrop\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m ref\u001b[38;5;241m=\u001b[39m\u001b[43mmodcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#when calculating our image quality metrics later we have the same size image to what we produce in SRCNN network\u001b[39;00m\n\u001b[0;32m     17\u001b[0m degraded\u001b[38;5;241m=\u001b[39mmodcrop(degraded,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#convert the image to YCrCb(3 channel image) - (srcnn trained on Y channel)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mmodcrop\u001b[1;34m(img, scale)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodcrop\u001b[39m(img,scale):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#temp size\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     tmpsz\u001b[38;5;241m=\u001b[39m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m      4\u001b[0m     sz\u001b[38;5;241m=\u001b[39mtmpsz[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#ensures that dimension of our image are divisible by scale(doesn't leaves hanging remainders) by cropping the images size\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#np.mod returns the remainder bewtween our sz and scale\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('bad/'):\n",
    "    \n",
    "    # perform super-resolution\n",
    "    ref, degraded, output = predict('bad/{}'.format(file))\n",
    "    \n",
    "    # display images as subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "    axs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))\n",
    "    axs[1].set_title('Degraded')\n",
    "    axs[1].set(xlabel = 'MSE: {}\\n'.format(scores[0][0]))\n",
    "    axs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "    axs[2].set_title('SRCNN')\n",
    "    axs[2].set(xlabel = 'MSE: {}\\n'.format(scores[1][0]))\n",
    "\n",
    "    # remove the x and y ticks\n",
    "    for ax in axs:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "      \n",
    "#     print('Saving {}'.format(file))\n",
    "#     fig.savefig('output/{}.png'.format(os.path.splitext(file)[0])) \n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a38c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23529fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
